{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.17136978, 0.26247924, 0.14671207],\n",
       "       [0.75039032, 0.53420895, 0.80619054],\n",
       "       [0.64236018, 0.04347309, 0.93404911],\n",
       "       [0.20756148, 0.03390594, 0.08715811]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate a random rectangular matrix\n",
    "A = np.random.rand(4, 3)\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is the maximum number of linearly independent rows or columns, while the trace of a matrix is the sum of its diagonal elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rank of A: 3\n"
     ]
    }
   ],
   "source": [
    "# Calculate the rank of A\n",
    "rank_A = np.linalg.matrix_rank(A)\n",
    "print(\"Rank of A:\", rank_A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6396278307865226\n"
     ]
    }
   ],
   "source": [
    "# Calculate the trace of A\n",
    "trace_A = np.trace(A)\n",
    "print(trace_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The determinant of a square matrix is a scalar value that can be used to understand properties of the matrix, such as invertibility and the scaling factor it applies to vectors when used in linear transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the determinant of A, we have to ensure that the matrix it's square, so we'll need another matrix from A, let's do that\n",
    "\n",
    "# Desired size for B (square matrix)\n",
    "N = 3\n",
    "\n",
    "# Truncate A to make it NxN\n",
    "B = A[:N, :N]\n",
    "\n",
    "det_B = np.linalg.det(B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New sqare matrix, calculated from the original matrix A\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.17136978, 0.26247924, 0.14671207],\n",
       "       [0.75039032, 0.53420895, 0.80619054],\n",
       "       [0.64236018, 0.04347309, 0.93404911]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's show the resulsts\n",
    "print(\"New sqare matrix, calculated from the original matrix A\")\n",
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Determinant of B (square matrix) -0.014098693157037559\n"
     ]
    }
   ],
   "source": [
    "print(\"Determinant of B (square matrix)\",det_B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can only invert a square matrix. A square matrix is one where the number of rows is equal to the number of columns (i.e., it has the shape NxN).\n",
    "\n",
    "If our matrix A is rectangular (i.e., it has a shape of NxM where N is not equal to M), we cannot directly invert it because it's not a square matrix. Inverting a non-square matrix is not a defined operation in linear algebra.\n",
    "\n",
    "If we want to compute the pseudo-inverse or perform other operations on a rectangular matrix, there are techniques like the Moore-Penrose pseudo-inverse (also known as the generalized inverse) that can be applied. However, these methods do not give us an \"inverse\" in the traditional sense, as they deal with non-square matrices and are used for specific purposes such as solving over-determined or under-determined linear systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear algebra, the singular value decomposition (SVD) is a factorization of a real or complex matrix. It generalizes the eigendecomposition of a square normal matrix with an orthonormal eigenbasis to any NxM matrix, so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pseudo inverse matrix of A: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-1.27693732,  0.23202888, -0.65994384,  7.07567635],\n",
       "       [ 1.60340416,  1.3410329 , -1.17177567, -2.54561239],\n",
       "       [ 0.59195896, -0.11023545,  1.52019663, -4.79490714]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pinv (psaudu-inverse) computes the Moore–Penrose pseudo inverse of a matrix using its SVD\n",
    "A_pseudo_inverse = np.linalg.pinv(A)\n",
    "\n",
    "print(\"Pseudo inverse matrix of A: \")\n",
    "A_pseudo_inverse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How are eigenvalues and eigenvectors of A’A and AA’ related? What interesting differences can you notice between both?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.0481616 , 0.48080918, 1.24818621],\n",
       "       [0.48080918, 0.35731407, 0.51274425],\n",
       "       [1.24818621, 0.51274425, 1.55151189]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate A^T A\n",
    "ATA = np.dot(A.T, A)\n",
    "ATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the eigenvalues and eigenvectors of ATA\n",
    "eigenvaluesATA, eigenvectorsATA = np.linalg.eig(ATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of ATA: [2.77652048 0.01193624 0.16853084]\n",
      "Eigenvectors of ATA: [[ 0.61202007  0.78801495 -0.06681226]\n",
      " [ 0.27851526 -0.29383788 -0.91437878]\n",
      " [ 0.74017612 -0.54100993  0.39930887]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalues and eigenvectors of ATA are now available\n",
    "print(\"Eigenvalues of ATA:\", eigenvaluesATA)\n",
    "print(\"Eigenvectors of ATA:\", eigenvectorsATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.11978738, 0.38709086, 0.25852818, 0.05725652],\n",
       "       [0.38709086, 1.49840801, 1.25826613, 0.24413103],\n",
       "       [0.25852818, 1.25826613, 1.28696425, 0.21621318],\n",
       "       [0.05725652, 0.24413103, 0.21621318, 0.05182792]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate AA^T\n",
    "AAT = np.dot(A,A.T)\n",
    "AAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the eigenvalues and eigenvectors of AAT\n",
    "eigenvaluesAAT, eigenvectorsAAT = np.linalg.eig(AAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of ATA: [ 2.77652048e+00  1.68530842e-01 -5.05283201e-17  1.19362356e-02]\n",
      "Eigenvectors of ATA: [[ 0.17198632  0.46981689  0.84327969 -0.19639795]\n",
      " [ 0.723021    0.52782539 -0.44538386 -0.016559  ]\n",
      " [ 0.65811261 -0.70715701  0.23435881 -0.10905369]\n",
      " [ 0.12061978  0.0245235   0.18864923  0.97430023]]\n"
     ]
    }
   ],
   "source": [
    "# Eigenvalues and eigenvectors for AAT are now available\n",
    "print(\"Eigenvalues of ATA:\", eigenvaluesAAT)\n",
    "print(\"Eigenvectors of ATA:\", eigenvectorsAAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see all of them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eigenvalues of ATA: [2.77652048 0.01193624 0.16853084]\n",
      "Eigenvectors of ATA: [[ 0.61202007  0.78801495 -0.06681226]\n",
      " [ 0.27851526 -0.29383788 -0.91437878]\n",
      " [ 0.74017612 -0.54100993  0.39930887]]\n",
      "Eigenvalues of AAT: [ 2.77652048e+00  1.68530842e-01 -5.05283201e-17  1.19362356e-02]\n",
      "Eigenvectors of AAT: [[ 0.17198632  0.46981689  0.84327969 -0.19639795]\n",
      " [ 0.723021    0.52782539 -0.44538386 -0.016559  ]\n",
      " [ 0.65811261 -0.70715701  0.23435881 -0.10905369]\n",
      " [ 0.12061978  0.0245235   0.18864923  0.97430023]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Eigenvalues of ATA:\", eigenvaluesATA)\n",
    "print(\"Eigenvectors of ATA:\", eigenvectorsATA)\n",
    "print(\"Eigenvalues of AAT:\", eigenvaluesAAT)\n",
    "print(\"Eigenvectors of AAT:\", eigenvectorsAAT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors of A^TA and AA^T\n",
    "\n",
    "The eigenvalues and eigenvectors of `A^TA` and `AA^T` are related, and there are some interesting differences between the two:\n",
    "\n",
    "1. **Eigenvalues**: The eigenvalues of `A^TA` and `AA^T` are related. Specifically, the non-zero eigenvalues of `A^TA` are the same as the non-zero eigenvalues of `AA^T`. In this case, we can see that both matrices have the same non-zero eigenvalues, which are approximately 2.77652048 and 0.16853084.\n",
    "\n",
    "2. **Number of Eigenvalues**: The number of non-zero eigenvalues of `A^TA` and `AA^T` is the same and equal to the rank of the original matrix `A`. However, `A^TA` and `AA^T` can have additional zero eigenvalues, and the number of these zero eigenvalues is the difference between the number of rows and columns (for `A^TA`) or columns and rows (for `AA^T`).\n",
    "\n",
    "3. **Eigenvectors**: The eigenvectors of `A^TA` and `AA^T` are different. The eigenvectors of `A^TA` are in the column space of `A`, while the eigenvectors of `AA^T` are in the row space of `A`. This means that they describe different linear relationships in the data.\n",
    "\n",
    "4. **Orthogonality**: The eigenvectors of `A^TA` are orthogonal (perpendicular) to each other because `A^TA` is a symmetric matrix. On the other hand, the eigenvectors of `AA^T` may not be orthogonal unless `A` has a special structure (e.g., orthogonal columns).\n",
    "\n",
    "In summary, while the eigenvalues of `A^TA` and `AA^T` are related, they describe different aspects of the data and have different eigenvectors. The choice of whether to use `A^TA` or `AA^T` for eigenanalysis depends on the specific problem we are trying to solve and the properties of the data.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
