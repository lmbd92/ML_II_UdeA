**In your own words, describe what vector embeddings are and what they are useful for.** 

Vector embeddings are a way to represent words as vectors, where each dimension of the vector represents a feature of the word. This is useful because it allows us to represent words in a way that is easy to manipulate and compare. For example, we can use vector embeddings to find words that are similar to each other, or to find words that are related to a given word. It's useful for NLP tasks such as word similarity, word analogy, and word sense disambiguation.

**What are the main differences between word2vec and GloVe?**

The main difference between word2vec and GloVe is that word2vec uses a neural network to learn the vector embeddings, while GloVe uses a co-occurrence matrix. This means that word2vec is better at capturing the semantic meaning of words, while GloVe is better at capturing the syntactic meaning of words.

**What are the main differences between word2vec and fastText?**

The main difference between word2vec and fastText is that word2vec uses a neural network to learn the vector embeddings, while fastText uses a bag-of-words model. This means that word2vec is better at capturing the semantic meaning of words, while fastText is better at capturing the syntactic meaning of words.

**What are the main differences between GloVe and fastText?**

The main difference between GloVe and fastText is that GloVe uses a co-occurrence matrix to learn the vector embeddings, while fastText uses a bag-of-words model. This means that GloVe is better at capturing the semantic meaning of words, while fastText is better at capturing the syntactic meaning of words.